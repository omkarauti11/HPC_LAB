{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDyueZy6xDui"
      },
      "outputs": [],
      "source": [
        "# Problem 1: Vector Addition using CUDA\n",
        "# Problem Statement: Write a CUDA C program that performs element-wise addition of two vectors A and B of size N. The result of the addition should be stored in vector C.\n",
        "# Details:\n",
        "# •\tInitialize the vectors A and B with random numbers.\n",
        "# •\tThe output vector C[i] = A[i] + B[i], where i ranges from 0 to N-1.\n",
        "# •\tUse CUDA kernels to perform the computation in parallel.\n",
        "# •\tWrite the code for both serial (CPU-based) and parallel (CUDA-based) implementations.\n",
        "# •\tMeasure the execution time of both the serial and CUDA implementations for different values of N (e.g., N = 10^5, 10^6, 10^7).\n",
        "# Task:\n",
        "# •\tCalculate and report the speedup (i.e., the ratio of CPU execution time to GPU execution time).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vector_add.cu\n",
        "#include <iostream>\n",
        "#include <cstdlib>\n",
        "#include <ctime>\n",
        "#include <cuda_runtime.h>\n",
        "#include <chrono>\n",
        "#include <cmath>\n",
        "\n",
        "using namespace std;\n",
        "using namespace std::chrono;\n",
        "\n",
        "// CUDA error-checking macro\n",
        "#define cudaCheckError() { \\\n",
        "    cudaError_t e=cudaGetLastError(); \\\n",
        "    if(e!=cudaSuccess) { \\\n",
        "        cout << \"CUDA Error \" << cudaGetErrorString(e) << \" at line \" << __LINE__ << endl; \\\n",
        "        exit(1); \\\n",
        "    } \\\n",
        "}\n",
        "\n",
        "// CUDA Kernel for vector addition\n",
        "__global__ void vectorAddKernel(float* A, float* B, float* C, int N) {\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (i < N) {\n",
        "        C[i] = A[i] + B[i];\n",
        "    }\n",
        "}\n",
        "\n",
        "// CPU function for vector addition (serial implementation)\n",
        "void vectorAddCPU(const float* A, const float* B, float* C, int N) {\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "        C[i] = A[i] + B[i];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int sizes[] = {100000, 1000000, 10000000}; // Array sizes: 10^5, 10^6, 10^7\n",
        "\n",
        "    for (int N : sizes) {\n",
        "        cout << \"Array Size: \" << N << endl;\n",
        "        size_t size = N * sizeof(float);\n",
        "\n",
        "        // Allocate memory on host\n",
        "        float *h_A = new float[N];\n",
        "        float *h_B = new float[N];\n",
        "        float *h_C = new float[N];       // for CPU result\n",
        "        float *h_C_cuda = new float[N];  // for GPU result\n",
        "\n",
        "        // Initialize vectors with random values\n",
        "        srand(time(0));\n",
        "        for (int i = 0; i < N; ++i) {\n",
        "            h_A[i] = static_cast<float>(rand()) / RAND_MAX;\n",
        "            h_B[i] = static_cast<float>(rand()) / RAND_MAX;\n",
        "        }\n",
        "\n",
        "        // CPU computation timing using std::chrono\n",
        "        auto start = high_resolution_clock::now();\n",
        "        vectorAddCPU(h_A, h_B, h_C, N);\n",
        "        auto end = high_resolution_clock::now();\n",
        "        double cpu_time = duration<double>(end - start).count();\n",
        "        cout << \"CPU Execution Time: \" << cpu_time << \" seconds\" << endl;\n",
        "\n",
        "        // Allocate memory on device with error checking\n",
        "        float *d_A, *d_B, *d_C;\n",
        "        cudaMalloc((void**)&d_A, size);\n",
        "        cudaCheckError();\n",
        "        cudaMalloc((void**)&d_B, size);\n",
        "        cudaCheckError();\n",
        "        cudaMalloc((void**)&d_C, size);\n",
        "        cudaCheckError();\n",
        "\n",
        "        // Copy data from host to device\n",
        "        cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);\n",
        "        cudaCheckError();\n",
        "        cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);\n",
        "        cudaCheckError();\n",
        "\n",
        "        // Set up execution configuration\n",
        "        int blockSize = 256;\n",
        "        int numBlocks = (N + blockSize - 1) / blockSize;\n",
        "\n",
        "        // GPU computation timing using std::chrono\n",
        "        start = high_resolution_clock::now();\n",
        "        vectorAddKernel<<<numBlocks, blockSize>>>(d_A, d_B, d_C, N);\n",
        "        cudaDeviceSynchronize();  // Ensure kernel execution is complete\n",
        "        cudaCheckError();\n",
        "        end = high_resolution_clock::now();\n",
        "        double gpu_time = duration<double>(end - start).count();\n",
        "        cout << \"GPU Execution Time: \" << gpu_time << \" seconds\" << endl;\n",
        "\n",
        "        // Copy result back to host\n",
        "        cudaMemcpy(h_C_cuda, d_C, size, cudaMemcpyDeviceToHost);\n",
        "        cudaCheckError();\n",
        "\n",
        "        // Verify results and print first 10 elements from both CPU and GPU arrays\n",
        "        cout << \"First 10 elements of CPU result: \";\n",
        "        for (int i = 0; i < 10; ++i) {\n",
        "            cout << h_C[i] << \" \";\n",
        "        }\n",
        "        cout << endl;\n",
        "\n",
        "        cout << \"First 10 elements of GPU result: \";\n",
        "        for (int i = 0; i < 10; ++i) {\n",
        "            cout << h_C_cuda[i] << \" \";\n",
        "        }\n",
        "        cout << endl;\n",
        "\n",
        "        // Verify results with a tolerance\n",
        "        bool success = true;\n",
        "        for (int i = 0; i < N; ++i) {\n",
        "            if (fabs(h_C[i] - h_C_cuda[i]) > 1e-5) {\n",
        "                success = false;\n",
        "                break;\n",
        "            }\n",
        "        }\n",
        "\n",
        "        if (success) {\n",
        "            cout << \"Results are correct!\" << endl;\n",
        "        } else {\n",
        "            cout << \"Results do not match!\" << endl;\n",
        "        }\n",
        "\n",
        "        // Calculate speedup\n",
        "        float speedup = cpu_time / gpu_time;\n",
        "        cout << \"Speedup (CPU time / GPU time): \" << speedup << endl;\n",
        "\n",
        "        // Free memory\n",
        "        delete[] h_A;\n",
        "        delete[] h_B;\n",
        "        delete[] h_C;\n",
        "        delete[] h_C_cuda;\n",
        "        cudaFree(d_A);\n",
        "        cudaFree(d_B);\n",
        "        cudaFree(d_C);\n",
        "\n",
        "        cout << \"------------------------------\" << endl;\n",
        "    }\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xh1Juabzybq1",
        "outputId": "db645d3d-0061-4855-cbe7-bd4d097f6b8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting vector_add.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o vector_add vector_add.cu\n"
      ],
      "metadata": {
        "id": "UI4lJHYzzKbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./vector_add\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VT8Bt-1zLG7",
        "outputId": "cdf14a07-c415-4bb6-b8af-bb70d1816e72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Array Size: 100000\n",
            "CPU Execution Time: 0.000591968 seconds\n",
            "GPU Execution Time: 0.128523 seconds\n",
            "First 10 elements of CPU result: 0.453392 0.407359 0.681873 1.43805 1.39707 0.824351 1.38028 0.926508 1.744 1.10131 \n",
            "First 10 elements of GPU result: 0.453392 0.407359 0.681873 1.43805 1.39707 0.824351 1.38028 0.926508 1.744 1.10131 \n",
            "Results are correct!\n",
            "Speedup (CPU time / GPU time): 0.00460592\n",
            "------------------------------\n",
            "Array Size: 1000000\n",
            "CPU Execution Time: 0.00654924 seconds\n",
            "GPU Execution Time: 0.000119705 seconds\n",
            "First 10 elements of CPU result: 0.72232 1.52647 1.33181 1.0906 1.28171 0.989055 1.32843 1.36948 0.680937 0.449285 \n",
            "First 10 elements of GPU result: 0.72232 1.52647 1.33181 1.0906 1.28171 0.989055 1.32843 1.36948 0.680937 0.449285 \n",
            "Results are correct!\n",
            "Speedup (CPU time / GPU time): 54.7115\n",
            "------------------------------\n",
            "Array Size: 10000000\n",
            "CPU Execution Time: 0.0619308 seconds\n",
            "GPU Execution Time: 0.0005118 seconds\n",
            "First 10 elements of CPU result: 0.72232 1.52647 1.33181 1.0906 1.28171 0.989055 1.32843 1.36948 0.680937 0.449285 \n",
            "First 10 elements of GPU result: 0.72232 1.52647 1.33181 1.0906 1.28171 0.989055 1.32843 1.36948 0.680937 0.449285 \n",
            "Results are correct!\n",
            "Speedup (CPU time / GPU time): 121.006\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 2: Matrix Addition using CUDA\n",
        "# Problem Statement: Write a CUDA C program to perform element-wise addition of two matrices A and B of size M x N. The result of the addition should be stored in matrix C.\n",
        "# Details:\n",
        "# •\tInitialize the matrices A and B with random values.\n",
        "# •\tThe output matrix C[i][j] = A[i][j] + B[i][j] where i ranges from 0 to M-1 and j ranges from 0 to N-1.\n",
        "# •\tWrite code for both serial (CPU-based) and parallel (CUDA-based) implementations.\n",
        "# •\tMeasure the execution time of both implementations for various matrix sizes (e.g., 100x100, 500x500, 1000x1000).\n",
        "# Task:\n",
        "# •\tCalculate the speedup using the execution times of the CPU and GPU implementations.\n"
      ],
      "metadata": {
        "id": "hidt5CJu2usI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile matrix_add.cu\n",
        "#include <iostream>\n",
        "#include <cstdlib>\n",
        "#include <cuda.h>\n",
        "#include <chrono>\n",
        "\n",
        "using namespace std;\n",
        "using namespace std::chrono;\n",
        "\n",
        "// Function to add matrices on CPU\n",
        "void matrixAddCPU(float *A, float *B, float *C, int M, int N) {\n",
        "    for (int i = 0; i < M; i++) {\n",
        "        for (int j = 0; j < N; j++) {\n",
        "            C[i * N + j] = A[i * N + j] + B[i * N + j];\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "// CUDA kernel for matrix addition\n",
        "__global__ void matrixAddKernel(float *A, float *B, float *C, int M, int N) {\n",
        "    int i = blockIdx.y * blockDim.y + threadIdx.y; // row index\n",
        "    int j = blockIdx.x * blockDim.x + threadIdx.x; // column index\n",
        "\n",
        "    if (i < M && j < N) {\n",
        "        C[i * N + j] = A[i * N + j] + B[i * N + j];\n",
        "    }\n",
        "}\n",
        "\n",
        "// Function to check for CUDA errors\n",
        "void cudaCheckError() {\n",
        "    cudaError_t err = cudaGetLastError();\n",
        "    if (err != cudaSuccess) {\n",
        "        fprintf(stderr, \"CUDA Error: %s\\n\", cudaGetErrorString(err));\n",
        "        exit(EXIT_FAILURE);\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Matrix dimensions for testing\n",
        "    int sizes[3][2] = { {100, 100}, {500, 500}, {1000, 1000} };\n",
        "\n",
        "    for (int k = 0; k < 3; k++) {\n",
        "        int M = sizes[k][0];\n",
        "        int N = sizes[k][1];\n",
        "\n",
        "        // Allocate host memory\n",
        "        float *h_A = (float *)malloc(M * N * sizeof(float));\n",
        "        float *h_B = (float *)malloc(M * N * sizeof(float));\n",
        "        float *h_C = (float *)malloc(M * N * sizeof(float));\n",
        "\n",
        "        // Initialize matrices A and B with random values\n",
        "        for (int i = 0; i < M * N; i++) {\n",
        "            h_A[i] = static_cast<float>(rand() % 100) / 10.0; // random values between 0 and 10\n",
        "            h_B[i] = static_cast<float>(rand() % 100) / 10.0; // random values between 0 and 10\n",
        "        }\n",
        "\n",
        "        // CPU Matrix Addition\n",
        "        auto start = high_resolution_clock::now();\n",
        "        matrixAddCPU(h_A, h_B, h_C, M, N);\n",
        "        auto cpu_time = duration_cast<duration<double>>(high_resolution_clock::now() - start).count();\n",
        "\n",
        "        // Allocate device memory\n",
        "        float *d_A, *d_B, *d_C;\n",
        "        cudaMalloc((void **)&d_A, M * N * sizeof(float));\n",
        "        cudaMalloc((void **)&d_B, M * N * sizeof(float));\n",
        "        cudaMalloc((void **)&d_C, M * N * sizeof(float));\n",
        "        cudaCheckError();\n",
        "\n",
        "        // Copy matrices from host to device\n",
        "        cudaMemcpy(d_A, h_A, M * N * sizeof(float), cudaMemcpyHostToDevice);\n",
        "        cudaMemcpy(d_B, h_B, M * N * sizeof(float), cudaMemcpyHostToDevice);\n",
        "        cudaCheckError();\n",
        "\n",
        "        // Define the number of blocks and threads\n",
        "        dim3 threadsPerBlock(16, 16);\n",
        "        dim3 numBlocks((N + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
        "                       (M + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
        "\n",
        "        // GPU Matrix Addition\n",
        "        start = high_resolution_clock::now();\n",
        "        matrixAddKernel<<<numBlocks, threadsPerBlock>>>(d_A, d_B, d_C, M, N);\n",
        "        cudaDeviceSynchronize(); // Wait for GPU to finish\n",
        "        auto gpu_time = duration_cast<duration<double>>(high_resolution_clock::now() - start).count();\n",
        "\n",
        "        // Copy result back to host\n",
        "        cudaMemcpy(h_C, d_C, M * N * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "        cudaCheckError();\n",
        "\n",
        "        // Validate results (checking first 10 elements)\n",
        "        int correct = 1;\n",
        "        for (int i = 0; i < 10; i++) {\n",
        "            if (h_C[i] != h_A[i] + h_B[i]) {\n",
        "                correct = 0;\n",
        "                break;\n",
        "            }\n",
        "        }\n",
        "\n",
        "        // Print results\n",
        "        printf(\"Matrix Size: %dx%d\\n\", M, N);\n",
        "        printf(\"CPU Execution Time: %.6f seconds\\n\", cpu_time);\n",
        "        printf(\"GPU Execution Time: %.6f seconds\\n\", gpu_time);\n",
        "        printf(\"First 10 elements of CPU result: \");\n",
        "        for (int i = 0; i < 10; i++) {\n",
        "            printf(\"%.6f \", h_C[i]);\n",
        "        }\n",
        "        printf(\"\\nFirst 10 elements of GPU result: \");\n",
        "        for (int i = 0; i < 10; i++) {\n",
        "            printf(\"%.6f \", h_C[i]);\n",
        "        }\n",
        "        printf(\"\\nResults are %s!\\n\", correct ? \"correct\" : \"not correct\");\n",
        "        printf(\"Speedup (CPU time / GPU time): %.2f\\n\", cpu_time / gpu_time);\n",
        "        printf(\"------------------------------\\n\");\n",
        "\n",
        "        // Free device memory\n",
        "        cudaFree(d_A);\n",
        "        cudaFree(d_B);\n",
        "        cudaFree(d_C);\n",
        "        cudaCheckError();\n",
        "\n",
        "        // Free host memory\n",
        "        free(h_A);\n",
        "        free(h_B);\n",
        "        free(h_C);\n",
        "    }\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQKtY8Oj27_G",
        "outputId": "6b2694c0-e5fb-4a20-f290-123f9c11ffbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting matrix_add.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o matrix_add matrix_add.cu\n"
      ],
      "metadata": {
        "id": "Sjf6lydJ3A4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./matrix_add\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTqpnk9l3BBU",
        "outputId": "1d15202a-b887-475d-d4e0-4569f61cf0f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix Size: 100x100\n",
            "CPU Execution Time: 0.000082 seconds\n",
            "GPU Execution Time: 0.038830 seconds\n",
            "First 10 elements of CPU result: 16.900002 9.200000 12.800000 17.799999 7.000000 8.900000 14.900000 8.900000 6.600000 10.799999 \n",
            "First 10 elements of GPU result: 16.900002 9.200000 12.800000 17.799999 7.000000 8.900000 14.900000 8.900000 6.600000 10.799999 \n",
            "Results are correct!\n",
            "Speedup (CPU time / GPU time): 0.00\n",
            "------------------------------\n",
            "Matrix Size: 500x500\n",
            "CPU Execution Time: 0.000882 seconds\n",
            "GPU Execution Time: 0.000103 seconds\n",
            "First 10 elements of CPU result: 15.000000 9.800000 3.300000 15.299999 5.200000 11.100000 6.800000 10.400000 10.000000 8.400000 \n",
            "First 10 elements of GPU result: 15.000000 9.800000 3.300000 15.299999 5.200000 11.100000 6.800000 10.400000 10.000000 8.400000 \n",
            "Results are correct!\n",
            "Speedup (CPU time / GPU time): 8.58\n",
            "------------------------------\n",
            "Matrix Size: 1000x1000\n",
            "CPU Execution Time: 0.005269 seconds\n",
            "GPU Execution Time: 0.000123 seconds\n",
            "First 10 elements of CPU result: 10.600000 9.600000 16.100000 7.000000 7.600000 17.500000 7.100000 11.800000 1.900000 7.500000 \n",
            "First 10 elements of GPU result: 10.600000 9.600000 16.100000 7.000000 7.600000 17.500000 7.100000 11.800000 1.900000 7.500000 \n",
            "Results are correct!\n",
            "Speedup (CPU time / GPU time): 42.80\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 3: Dot Product of Two Vectors using CUDA\n",
        "# Problem Statement: Write a CUDA C program to compute the dot product of two vectors A and B of size N. The dot product is defined as:\n",
        "# Details:\n",
        "# •\tInitialize the vectors A and B with random values.\n",
        "# •\tImplement the dot product calculation using both serial (CPU) and parallel (CUDA) approaches.\n",
        "# •\tMeasure the execution time for both implementations with different vector sizes (e.g., N = 10^5, 10^6, 10^7).\n",
        "# •\tUse atomic operations or shared memory reduction in the CUDA kernel to compute the final sum.\n",
        "# Task:\n",
        "# •\tCalculate and report the speedup for different vector sizes.\n"
      ],
      "metadata": {
        "id": "9SZQBwVI5YpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dot_product.cu\n",
        "#include <iostream>\n",
        "#include <chrono>\n",
        "#include <cstdlib>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "// CUDA kernel for calculating dot product\n",
        "__global__ void dotProductKernel(int *A, int *B, int *C, int N) {\n",
        "    extern __shared__ int sharedData[]; // Dynamic shared memory\n",
        "    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    int localIndex = threadIdx.x;\n",
        "\n",
        "    // Initialize shared memory\n",
        "    if (tid < N) {\n",
        "        sharedData[localIndex] = A[tid] * B[tid];\n",
        "    } else {\n",
        "        sharedData[localIndex] = 0; // Ensure unused threads contribute 0\n",
        "    }\n",
        "\n",
        "    __syncthreads(); // Synchronize threads within the block\n",
        "\n",
        "    // Perform reduction in shared memory\n",
        "    for (int stride = blockDim.x / 2; stride > 0; stride /= 2) {\n",
        "        if (localIndex < stride) {\n",
        "            sharedData[localIndex] += sharedData[localIndex + stride];\n",
        "        }\n",
        "        __syncthreads(); // Synchronize after each reduction step\n",
        "    }\n",
        "\n",
        "    // Write result of this block to global memory\n",
        "    if (localIndex == 0) {\n",
        "        atomicAdd(C, sharedData[0]); // Use atomic operation to avoid race conditions\n",
        "    }\n",
        "}\n",
        "\n",
        "// CPU function to calculate dot product\n",
        "int dotProductCPU(int *A, int *B, int N) {\n",
        "    int sum = 0;\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        sum += A[i] * B[i];\n",
        "    }\n",
        "    return sum;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int sizes[] = {100000, 1000000, 10000000}; // Different sizes\n",
        "    for (int s = 0; s < 3; s++) {\n",
        "        int N = sizes[s];\n",
        "        int *A, *B, *C; // Host variables\n",
        "        int *d_A, *d_B, *d_C; // Device variables\n",
        "\n",
        "        // Allocate memory on host\n",
        "        A = (int*)malloc(N * sizeof(int));\n",
        "        B = (int*)malloc(N * sizeof(int));\n",
        "        C = (int*)malloc(sizeof(int)); // Single int for result\n",
        "\n",
        "        // Initialize vectors with random values\n",
        "        for (int i = 0; i < N; i++) {\n",
        "            A[i] = rand() % 100; // Random integers between 0 and 99\n",
        "            B[i] = rand() % 100; // Random integers between 0 and 99\n",
        "        }\n",
        "\n",
        "        // Allocate memory on device\n",
        "        cudaMalloc((void**)&d_A, N * sizeof(int));\n",
        "        cudaMalloc((void**)&d_B, N * sizeof(int));\n",
        "        cudaMalloc((void**)&d_C, sizeof(int));\n",
        "\n",
        "        // Copy vectors from host to device\n",
        "        cudaMemcpy(d_A, A, N * sizeof(int), cudaMemcpyHostToDevice);\n",
        "        cudaMemcpy(d_B, B, N * sizeof(int), cudaMemcpyHostToDevice);\n",
        "        cudaMemcpy(d_C, C, sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "        // Measure CPU execution time\n",
        "        auto startCPU = chrono::high_resolution_clock::now();\n",
        "        int resultCPU = dotProductCPU(A, B, N);\n",
        "        auto endCPU = chrono::high_resolution_clock::now();\n",
        "        auto cpuDuration = chrono::duration_cast<chrono::microseconds>(endCPU - startCPU).count();\n",
        "\n",
        "        // Measure GPU execution time\n",
        "        int initialValue = 0;\n",
        "        cudaMemcpy(d_C, &initialValue, sizeof(int), cudaMemcpyHostToDevice); // Initialize C on device\n",
        "\n",
        "        int blockSize = 256; // Define block size\n",
        "        int numBlocks = (N + blockSize - 1) / blockSize; // Calculate number of blocks\n",
        "\n",
        "        auto startGPU = chrono::high_resolution_clock::now();\n",
        "        dotProductKernel<<<numBlocks, blockSize, blockSize * sizeof(int)>>>(d_A, d_B, d_C, N);\n",
        "        cudaDeviceSynchronize(); // Wait for GPU to finish\n",
        "        auto endGPU = chrono::high_resolution_clock::now();\n",
        "        cudaMemcpy(C, d_C, sizeof(int), cudaMemcpyDeviceToHost); // Copy result from device to host\n",
        "        auto gpuDuration = chrono::duration_cast<chrono::microseconds>(endGPU - startGPU).count();\n",
        "\n",
        "        // Display results\n",
        "        cout << \"Vector Size: \" << N << endl;\n",
        "        cout << \"CPU Execution Time: \" << cpuDuration << \" microseconds\" << endl;\n",
        "        cout << \"GPU Execution Time: \" << gpuDuration << \" microseconds\" << endl;\n",
        "        cout << \"CPU Result: \" << resultCPU << endl;\n",
        "        cout << \"GPU Result: \" << *C << endl;\n",
        "\n",
        "        if (abs(resultCPU - *C) < 1e-5) {\n",
        "            cout << \"Results are correct!\" << endl;\n",
        "        } else {\n",
        "            cout << \"Results do not match!\" << endl;\n",
        "        }\n",
        "\n",
        "        float speedup = static_cast<float>(cpuDuration) / gpuDuration;\n",
        "        cout << \"Speedup (CPU time / GPU time): \" << speedup << endl;\n",
        "        cout << \"------------------------------\" << endl;\n",
        "\n",
        "        // Free memory\n",
        "        free(A);\n",
        "        free(B);\n",
        "        free(C);\n",
        "        cudaFree(d_A);\n",
        "        cudaFree(d_B);\n",
        "        cudaFree(d_C);\n",
        "    }\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAuP39rc4_k3",
        "outputId": "21681799-584e-41a6-de74-67781fbb746b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting dot_product.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o dot_product dot_product.cu\n"
      ],
      "metadata": {
        "id": "DuTLOTQg4_xG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./dot_product\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEzum2k-4_7p",
        "outputId": "551fb06e-b9b3-4bc0-aefb-e67f25318522"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector Size: 100000\n",
            "CPU Execution Time: 269 microseconds\n",
            "GPU Execution Time: 40012 microseconds\n",
            "CPU Result: 245034401\n",
            "GPU Result: 245034401\n",
            "Results are correct!\n",
            "Speedup (CPU time / GPU time): 0.00672298\n",
            "------------------------------\n",
            "Vector Size: 1000000\n",
            "CPU Execution Time: 2667 microseconds\n",
            "GPU Execution Time: 122 microseconds\n",
            "CPU Result: -1844440173\n",
            "GPU Result: -1844440173\n",
            "Results are correct!\n",
            "Speedup (CPU time / GPU time): 21.8607\n",
            "------------------------------\n",
            "Vector Size: 10000000\n",
            "CPU Execution Time: 25390 microseconds\n",
            "GPU Execution Time: 1035 microseconds\n",
            "CPU Result: -1259994077\n",
            "GPU Result: -1259994077\n",
            "Results are correct!\n",
            "Speedup (CPU time / GPU time): 24.5314\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem Statement: Write a CUDA C program to perform matrix multiplication. Given two matrices A (MxN) and B (NxP), compute the resulting matrix C (MxP) where:\n",
        "# Details:\n",
        "# •\tInitialize the matrices A and B with random values.\n",
        "# •\tWrite code for both serial (CPU-based) and parallel (CUDA-based) implementations.\n",
        "# •\tMeasure the execution time of both implementations for various matrix sizes (e.g., 100x100, 500x500, 1000x1000).\n",
        "# Task:\n",
        "# •\tCalculate the speedup by comparing the CPU and GPU execution times\n"
      ],
      "metadata": {
        "id": "z7f8ZR9q6l5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile matrix_multiplication.cu\n",
        "#include <iostream>\n",
        "#include <cstdlib>\n",
        "#include <ctime>\n",
        "#include <chrono>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "// Function to multiply matrices on CPU\n",
        "void matrixMultiplyCPU(int* A, int* B, int* C, int M, int N, int P) {\n",
        "    for (int i = 0; i < M; i++) {\n",
        "        for (int j = 0; j < P; j++) {\n",
        "            C[i * P + j] = 0;\n",
        "            for (int k = 0; k < N; k++) {\n",
        "                C[i * P + j] += A[i * N + k] * B[k * P + j];\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "// CUDA kernel for matrix multiplication on GPU\n",
        "__global__ void matrixMultiplyGPU(int* A, int* B, int* C, int M, int N, int P) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < M && col < P) {\n",
        "        int sum = 0;\n",
        "        for (int k = 0; k < N; k++) {\n",
        "            sum += A[row * N + k] * B[k * P + col];\n",
        "        }\n",
        "        C[row * P + col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "// Function to initialize a matrix with random integer values\n",
        "void initializeMatrix(int* matrix, int rows, int cols) {\n",
        "    for (int i = 0; i < rows * cols; i++) {\n",
        "        matrix[i] = rand() % 10; // Random integer values between 0 and 9\n",
        "    }\n",
        "}\n",
        "\n",
        "// Function to print the first row of a matrix\n",
        "void printFirstRow(const int* matrix, int cols) {\n",
        "    for (int j = 0; j < cols; j++) {\n",
        "        cout << matrix[j] << \" \";\n",
        "    }\n",
        "    cout << endl;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    srand(static_cast<unsigned int>(time(0)));\n",
        "\n",
        "    // Define matrix sizes\n",
        "    int sizes[][2] = {{100, 100}, {500, 500}, {1000, 1000}};\n",
        "    int numTests = 3;\n",
        "\n",
        "    for (int t = 0; t < numTests; t++) {\n",
        "        int M = sizes[t][0];\n",
        "        int N = sizes[t][1];\n",
        "        int P = sizes[t][1];\n",
        "\n",
        "        // Allocate host memory for matrices\n",
        "        int* A = (int*)malloc(M * N * sizeof(int));\n",
        "        int* B = (int*)malloc(N * P * sizeof(int));\n",
        "        int* C_CPU = (int*)malloc(M * P * sizeof(int));\n",
        "        int* C_GPU = (int*)malloc(M * P * sizeof(int));\n",
        "\n",
        "        // Initialize matrices A and B with random values\n",
        "        initializeMatrix(A, M, N);\n",
        "        initializeMatrix(B, N, P);\n",
        "\n",
        "        // --- CPU Matrix Multiplication ---\n",
        "        auto startCPU = chrono::high_resolution_clock::now();\n",
        "        matrixMultiplyCPU(A, B, C_CPU, M, N, P);\n",
        "        auto endCPU = chrono::high_resolution_clock::now();\n",
        "        chrono::duration<double> cpuDuration = endCPU - startCPU;\n",
        "\n",
        "        // --- GPU Matrix Multiplication ---\n",
        "        int *d_A, *d_B, *d_C;\n",
        "        cudaMalloc((void**)&d_A, M * N * sizeof(int));\n",
        "        cudaMalloc((void**)&d_B, N * P * sizeof(int));\n",
        "        cudaMalloc((void**)&d_C, M * P * sizeof(int));\n",
        "\n",
        "        // Copy matrices A and B to device memory\n",
        "        cudaMemcpy(d_A, A, M * N * sizeof(int), cudaMemcpyHostToDevice);\n",
        "        cudaMemcpy(d_B, B, N * P * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "        // Define CUDA grid and block dimensions\n",
        "        dim3 threadsPerBlock(16, 16);\n",
        "        dim3 numBlocks((P + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
        "                       (M + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
        "\n",
        "        // Launch the CUDA kernel\n",
        "        auto startGPU = chrono::high_resolution_clock::now();\n",
        "        matrixMultiplyGPU<<<numBlocks, threadsPerBlock>>>(d_A, d_B, d_C, M, N, P);\n",
        "        cudaDeviceSynchronize(); // Ensure all threads have completed\n",
        "        auto endGPU = chrono::high_resolution_clock::now();\n",
        "        chrono::duration<double> gpuDuration = endGPU - startGPU;\n",
        "\n",
        "        // Copy result matrix C from device to host\n",
        "        cudaMemcpy(C_GPU, d_C, M * P * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "        // Print first row of result matrices after CPU and GPU calculations\n",
        "        cout << \"\\nFirst Row of Result Matrix (CPU) for size \" << M << \"x\" << P << \":\\n\";\n",
        "        printFirstRow(C_CPU, P);\n",
        "\n",
        "        cout << \"\\nFirst Row of Result Matrix (GPU) for size \" << M << \"x\" << P << \":\\n\";\n",
        "        printFirstRow(C_GPU, P);\n",
        "\n",
        "        // Free device memory\n",
        "        cudaFree(d_A);\n",
        "        cudaFree(d_B);\n",
        "        cudaFree(d_C);\n",
        "\n",
        "        // Output timing results\n",
        "        cout << \"Matrix Size: \" << M << \"x\" << N << \" x \" << N << \"x\" << P << \" -> \" << M << \"x\" << P << endl;\n",
        "        cout << \"CPU Execution Time: \" << cpuDuration.count() * 1e6 << \" microseconds\" << endl;\n",
        "        cout << \"GPU Execution Time: \" << gpuDuration.count() * 1e6 << \" microseconds\" << endl;\n",
        "\n",
        "        // Calculate and print speedup\n",
        "        if (gpuDuration.count() > 0) {\n",
        "            double speedup = cpuDuration.count() / gpuDuration.count();\n",
        "            cout << \"Speedup (CPU time / GPU time): \" << speedup << endl;\n",
        "        }\n",
        "\n",
        "        cout << \"------------------------------\" << endl;\n",
        "\n",
        "        // Free host memory\n",
        "        free(A);\n",
        "        free(B);\n",
        "        free(C_CPU);\n",
        "        free(C_GPU);\n",
        "    }\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLKRn8BL6l8a",
        "outputId": "abf566d5-e30d-420e-e969-6e4a90832df3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting matrix_multiplication.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o matrix_multiplication matrix_multiplication.cu\n"
      ],
      "metadata": {
        "id": "kq6Mc-5c6l_M"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./matrix_multiplication\n"
      ],
      "metadata": {
        "id": "OAwJEHWn6mB4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8035436-962b-42e5-e29c-db67a8beb885"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "First Row of Result Matrix (CPU) for size 100x100:\n",
            "2257 1961 2160 2282 2447 2107 2124 2114 2304 2333 2141 2351 2079 2568 2195 2227 2161 2191 2111 2498 2283 2345 2365 2663 1875 2396 2205 2269 2165 1870 2027 2312 2363 2453 2322 2073 2214 1958 2228 2247 2267 2178 2311 2361 1927 2386 2166 2069 2065 2133 2536 2162 2314 2297 2296 2321 1902 2227 2360 2153 2217 1847 2302 2330 2297 2302 1910 1886 2368 2164 2265 2034 2514 2256 2310 2491 2169 2200 2238 2201 2175 2119 2299 2389 2090 2262 2265 2205 2339 2170 2199 2164 2207 1972 2009 2174 2176 1957 2412 2155 \n",
            "\n",
            "First Row of Result Matrix (GPU) for size 100x100:\n",
            "2257 1961 2160 2282 2447 2107 2124 2114 2304 2333 2141 2351 2079 2568 2195 2227 2161 2191 2111 2498 2283 2345 2365 2663 1875 2396 2205 2269 2165 1870 2027 2312 2363 2453 2322 2073 2214 1958 2228 2247 2267 2178 2311 2361 1927 2386 2166 2069 2065 2133 2536 2162 2314 2297 2296 2321 1902 2227 2360 2153 2217 1847 2302 2330 2297 2302 1910 1886 2368 2164 2265 2034 2514 2256 2310 2491 2169 2200 2238 2201 2175 2119 2299 2389 2090 2262 2265 2205 2339 2170 2199 2164 2207 1972 2009 2174 2176 1957 2412 2155 \n",
            "Matrix Size: 100x100 x 100x100 -> 100x100\n",
            "CPU Execution Time: 4295.14 microseconds\n",
            "GPU Execution Time: 44280.3 microseconds\n",
            "Speedup (CPU time / GPU time): 0.0969987\n",
            "------------------------------\n",
            "\n",
            "First Row of Result Matrix (CPU) for size 500x500:\n",
            "9767 9565 9417 9612 9442 9731 9745 9546 9374 8959 10054 9680 9516 9121 8968 9472 9818 9051 9680 9555 9465 9697 10127 10078 10365 9489 10206 10417 9834 9009 9133 9617 10352 10009 10067 9710 9598 9578 9732 9553 9389 9844 10241 9421 9830 9600 9288 9856 9585 10006 9635 9795 9850 9619 10098 9681 9665 10121 10050 9414 9555 9342 9890 9714 9710 9656 9555 9328 9229 8905 9737 9176 9883 10011 9638 9542 9310 9236 10233 9495 9519 9640 9103 10101 9699 9198 9622 9482 9665 9445 9211 9097 9062 9364 9943 9275 9114 9068 9349 9877 9785 9767 10142 9993 9427 9663 9816 10299 10156 9214 9674 10034 9310 9588 9407 9172 9325 9664 9771 9208 9332 10125 9428 9623 10030 9423 9150 9273 9535 9509 9599 9712 9244 9785 9703 9946 9633 9873 9663 9685 9446 10109 9527 9870 9457 9495 9518 8992 9326 9276 9851 10192 9982 10043 9543 9765 9066 9597 9522 10208 9898 9731 9777 9601 10182 9636 9671 9479 9376 10232 9763 8979 9734 9002 9362 9625 9568 9587 9608 9208 9540 9201 9478 9957 9557 9636 9371 9609 9799 10117 9727 9371 9452 9347 9702 9638 9960 9487 9594 9605 9406 9867 9895 9277 9134 9446 9481 9711 9826 9626 9372 9687 9378 10025 9399 9996 9599 8977 9851 9786 9652 10062 9179 9374 9962 9114 10261 10411 9328 9891 9107 9882 9127 9288 9770 9727 9682 9586 9503 9437 9817 9935 9305 9921 9381 9636 9210 9781 9931 8595 9453 9463 9818 9957 10289 10173 9475 9363 9631 9337 8746 9857 9581 9322 9909 9352 10331 9368 9907 10013 9320 9360 9528 9843 9170 9056 9583 9841 9422 9225 9618 10066 9921 9769 9864 9203 9235 9903 9350 9600 9322 8998 9345 9787 9860 10131 9558 9992 9560 9594 9653 9193 9630 9672 9386 9442 9491 9300 10409 10087 9697 9838 9780 9847 9534 9546 9630 9817 9210 9925 9973 9796 9757 9150 9806 9361 9510 9868 9704 10892 9478 9041 9704 10255 9506 10054 9659 9536 9989 9606 9385 10718 9861 10182 9786 9395 9270 10034 9731 9826 9870 8870 10042 9805 9679 9661 9632 9632 9637 9798 9903 9804 9804 9336 10020 9718 9952 9380 9612 10049 9195 8978 9945 9933 9995 9511 9270 9888 9984 10158 10120 10300 9317 9949 9191 9418 9928 8929 9153 9950 9436 9494 9094 9920 8892 9567 9819 9934 9524 9814 9274 9456 9411 10297 9540 9406 9626 9301 9782 9741 9447 9283 9817 9852 9454 10566 9348 9737 9814 9889 9419 9754 9659 9453 9398 10262 9994 9886 10137 9561 9194 10151 9722 9670 9627 9586 9694 9969 9418 9705 9515 9648 9536 10116 9263 10071 9939 9944 9719 10214 9872 9739 9844 10182 10143 9768 9791 9637 9087 9359 9379 10036 9641 9239 9116 9898 9796 9392 9924 9974 10358 9879 9446 9642 10027 10018 9291 10075 9747 9343 9871 9680 9414 9532 9986 9997 9703 9702 9260 9407 10340 9921 10085 9794 9293 9944 9634 9567 10094 9985 \n",
            "\n",
            "First Row of Result Matrix (GPU) for size 500x500:\n",
            "9767 9565 9417 9612 9442 9731 9745 9546 9374 8959 10054 9680 9516 9121 8968 9472 9818 9051 9680 9555 9465 9697 10127 10078 10365 9489 10206 10417 9834 9009 9133 9617 10352 10009 10067 9710 9598 9578 9732 9553 9389 9844 10241 9421 9830 9600 9288 9856 9585 10006 9635 9795 9850 9619 10098 9681 9665 10121 10050 9414 9555 9342 9890 9714 9710 9656 9555 9328 9229 8905 9737 9176 9883 10011 9638 9542 9310 9236 10233 9495 9519 9640 9103 10101 9699 9198 9622 9482 9665 9445 9211 9097 9062 9364 9943 9275 9114 9068 9349 9877 9785 9767 10142 9993 9427 9663 9816 10299 10156 9214 9674 10034 9310 9588 9407 9172 9325 9664 9771 9208 9332 10125 9428 9623 10030 9423 9150 9273 9535 9509 9599 9712 9244 9785 9703 9946 9633 9873 9663 9685 9446 10109 9527 9870 9457 9495 9518 8992 9326 9276 9851 10192 9982 10043 9543 9765 9066 9597 9522 10208 9898 9731 9777 9601 10182 9636 9671 9479 9376 10232 9763 8979 9734 9002 9362 9625 9568 9587 9608 9208 9540 9201 9478 9957 9557 9636 9371 9609 9799 10117 9727 9371 9452 9347 9702 9638 9960 9487 9594 9605 9406 9867 9895 9277 9134 9446 9481 9711 9826 9626 9372 9687 9378 10025 9399 9996 9599 8977 9851 9786 9652 10062 9179 9374 9962 9114 10261 10411 9328 9891 9107 9882 9127 9288 9770 9727 9682 9586 9503 9437 9817 9935 9305 9921 9381 9636 9210 9781 9931 8595 9453 9463 9818 9957 10289 10173 9475 9363 9631 9337 8746 9857 9581 9322 9909 9352 10331 9368 9907 10013 9320 9360 9528 9843 9170 9056 9583 9841 9422 9225 9618 10066 9921 9769 9864 9203 9235 9903 9350 9600 9322 8998 9345 9787 9860 10131 9558 9992 9560 9594 9653 9193 9630 9672 9386 9442 9491 9300 10409 10087 9697 9838 9780 9847 9534 9546 9630 9817 9210 9925 9973 9796 9757 9150 9806 9361 9510 9868 9704 10892 9478 9041 9704 10255 9506 10054 9659 9536 9989 9606 9385 10718 9861 10182 9786 9395 9270 10034 9731 9826 9870 8870 10042 9805 9679 9661 9632 9632 9637 9798 9903 9804 9804 9336 10020 9718 9952 9380 9612 10049 9195 8978 9945 9933 9995 9511 9270 9888 9984 10158 10120 10300 9317 9949 9191 9418 9928 8929 9153 9950 9436 9494 9094 9920 8892 9567 9819 9934 9524 9814 9274 9456 9411 10297 9540 9406 9626 9301 9782 9741 9447 9283 9817 9852 9454 10566 9348 9737 9814 9889 9419 9754 9659 9453 9398 10262 9994 9886 10137 9561 9194 10151 9722 9670 9627 9586 9694 9969 9418 9705 9515 9648 9536 10116 9263 10071 9939 9944 9719 10214 9872 9739 9844 10182 10143 9768 9791 9637 9087 9359 9379 10036 9641 9239 9116 9898 9796 9392 9924 9974 10358 9879 9446 9642 10027 10018 9291 10075 9747 9343 9871 9680 9414 9532 9986 9997 9703 9702 9260 9407 10340 9921 10085 9794 9293 9944 9634 9567 10094 9985 \n",
            "Matrix Size: 500x500 x 500x500 -> 500x500\n",
            "CPU Execution Time: 1.13339e+06 microseconds\n",
            "GPU Execution Time: 1121.28 microseconds\n",
            "Speedup (CPU time / GPU time): 1010.8\n",
            "------------------------------\n",
            "\n",
            "First Row of Result Matrix (CPU) for size 1000x1000:\n",
            "20241 21212 20030 20501 20238 20565 20398 20942 19959 19730 21249 21425 20758 20717 20743 20058 20744 20777 20120 21106 19980 21232 20855 20281 20346 20105 20365 21371 20349 21486 20765 20408 20087 20949 21731 20929 21293 20147 21412 20915 20583 21013 21217 19736 20052 21392 19800 20728 21893 20516 20953 20754 19818 20733 20778 20714 20046 20523 21435 21255 21112 21737 20781 20072 20028 20691 21298 20869 21178 19705 20656 20169 20151 21624 20490 21024 21766 20997 20837 21199 19779 20815 20970 20888 21232 20827 20671 20794 20295 19639 20834 20513 20997 20253 20378 19686 20744 20862 19892 20650 20310 21399 20385 20069 21663 20457 20263 21386 21153 20395 21654 20618 21024 21236 20493 20510 20618 21105 21115 19861 20366 21512 20505 20217 21257 20512 21669 21088 20898 21003 20934 21070 21091 21144 21167 20589 20946 20859 20469 20940 20678 20548 20386 21083 21051 20597 21301 20975 20484 20066 20890 21226 20435 20591 20874 19924 20598 21921 21056 21094 19984 20871 20873 21095 20783 20586 20297 21911 20955 21409 21420 21130 20268 21194 21125 20719 20593 20002 20515 21243 19928 20828 20844 19484 20612 19963 19963 20258 20812 21400 19900 21291 20492 20680 20229 21410 20783 20357 21172 20807 19984 21339 20835 20729 20251 20470 20044 20553 19964 21132 20665 20589 19918 21070 20159 21128 20841 20728 20539 20244 20808 20100 19300 20497 21035 20807 21209 21067 21449 20104 20784 20693 19898 20823 20869 20362 20102 20691 21343 21179 20119 20892 19731 21605 20146 20238 20927 20683 20268 20869 20449 20914 20893 20349 20374 20695 20922 20495 20790 21432 20569 21337 19650 21363 20236 20483 20972 20635 19070 19841 21179 21195 20230 22047 20336 21020 20167 20474 21599 21395 20073 20797 21022 20789 20821 21069 20962 20932 20888 20396 20560 20738 20019 20607 20648 20440 21062 20723 21759 20974 21027 20860 21041 20756 20429 21434 19796 20826 21191 20747 20379 20516 20816 20524 20548 20341 21393 20711 20837 19191 20798 20928 21138 21081 20990 21118 20950 21433 21166 20766 20813 20801 20328 21397 20171 20552 20854 20513 20458 20287 21132 20625 20826 20809 20749 20371 21308 21664 21158 20530 20191 19803 20958 20490 20666 21338 20640 20497 21353 20594 20589 20910 20763 21249 20817 20688 21106 20464 20932 20771 20465 20175 20918 20705 20511 20251 19989 20746 20652 20647 20959 20435 20457 21402 20973 21138 21051 20183 19886 20924 21119 20686 20959 20991 20912 20210 19786 20928 21260 20915 21547 21335 21100 21242 21148 21445 20259 21012 20664 20873 20876 20236 21050 19550 20345 21837 21182 21174 21065 21123 20733 21380 20416 20242 20753 20488 20484 21186 20937 20534 21161 18982 19706 20926 20593 20313 20677 20108 20516 19778 20875 20589 20005 21123 20155 19447 20597 20508 20881 20983 20584 21467 20843 21064 21211 20760 20554 20612 20172 20281 21614 20841 20960 20748 20722 21015 20738 20742 20354 21172 20811 20195 19871 20349 20757 19894 20551 20018 20900 20030 20143 21130 20697 21222 20699 20400 20748 21361 20623 19747 20554 20743 20517 20554 21376 20444 20799 20999 21414 21850 20965 19464 20343 20879 20345 20468 21108 20390 21002 20846 20159 20924 20849 20475 21246 20155 20524 20998 20548 20314 21577 20481 20238 20283 20056 20152 19831 19814 20944 20323 20978 19816 21602 20736 21094 20866 21130 19913 21593 19995 20940 21442 21201 21305 20665 20687 20794 21013 21091 19944 19924 21656 20953 20512 21475 20930 20270 20771 21379 20296 20292 20435 19992 20600 21170 20435 21241 20759 21022 20546 20430 20729 21035 20408 20888 20393 20678 21576 20177 19981 20881 20981 19975 21085 20249 20321 21302 20837 20479 20642 20533 22236 20493 20178 21540 20821 20570 20942 20325 20721 20697 20598 21069 21513 20568 20800 20182 21118 20866 21363 21062 20421 20384 20728 20261 20778 20653 19591 20876 19878 20739 20613 20057 21684 20572 21225 20681 20326 20689 20207 20757 21056 21236 20843 20914 20520 20382 20978 20413 20291 20194 20443 21056 21698 20046 20794 21151 21200 20881 21008 20336 20788 21157 20826 21245 21156 20088 19328 20069 20637 20330 19626 20578 20950 20407 21375 21087 21410 20201 20547 20892 19722 20771 20528 21033 20893 20148 20895 21068 20757 20934 20787 20942 21239 20775 20857 20794 20658 20466 21259 22002 20375 20805 21276 21501 21041 21304 21131 20950 20598 20247 21095 19843 20656 19692 20572 21134 20798 20633 20587 20799 20776 21457 21230 20858 19838 20597 20785 20670 21450 21685 21481 21394 21365 21635 21088 20252 20638 21665 20224 20893 21312 20657 20863 21148 20587 21210 20258 20484 21057 20940 20744 20248 20943 21445 20708 19894 20384 19658 20804 21339 20719 21419 21205 20570 20882 20473 20680 20430 20842 21223 20844 21141 20914 21322 20939 21024 20665 20584 20570 20156 21157 20038 19858 20482 21070 21325 20608 20254 21616 20871 20704 20637 20757 20753 21869 21274 20418 20899 21132 21488 20837 21258 20844 21379 20252 20079 21039 21363 20246 20078 20448 20911 20700 21037 20858 20512 21041 21178 20468 19681 20358 20679 20475 20462 21914 20980 20333 20692 21595 20127 20861 20849 20216 20266 21125 21360 20014 21243 20913 20929 20686 20632 21271 19830 20380 20687 20931 21402 21072 21498 20759 21451 20700 21172 20576 20818 19858 20261 20674 21128 21365 20456 20817 21615 20660 20443 20620 20299 21430 20875 20047 20397 20196 21010 20579 20943 20915 20338 19938 19631 21249 21202 20595 21368 20966 19735 20865 20754 20143 20946 19749 21090 20629 20629 20839 20700 20498 21155 20742 20920 20619 20353 20558 19836 20377 21010 20353 20982 21973 20657 19916 19911 20544 20040 20657 21505 20758 20326 20085 20195 20707 20392 19749 21117 20945 21070 21456 21188 20801 21171 20958 21139 20645 20381 20724 20649 21580 21304 20767 20609 21035 20694 20168 20743 19882 20588 21125 21564 20315 20973 20913 21229 20563 20787 20787 20825 21608 20290 20556 20661 21365 20880 20433 20184 21012 20751 20761 20690 20603 19694 20865 21017 20546 21605 20943 21390 20275 20883 20126 20405 21299 20868 20486 21362 20310 20710 20616 20826 21216 21858 20934 21435 20958 20790 20558 20505 20767 20873 20958 20078 20863 20279 21663 20981 21514 20509 20490 20340 20912 \n",
            "\n",
            "First Row of Result Matrix (GPU) for size 1000x1000:\n",
            "20241 21212 20030 20501 20238 20565 20398 20942 19959 19730 21249 21425 20758 20717 20743 20058 20744 20777 20120 21106 19980 21232 20855 20281 20346 20105 20365 21371 20349 21486 20765 20408 20087 20949 21731 20929 21293 20147 21412 20915 20583 21013 21217 19736 20052 21392 19800 20728 21893 20516 20953 20754 19818 20733 20778 20714 20046 20523 21435 21255 21112 21737 20781 20072 20028 20691 21298 20869 21178 19705 20656 20169 20151 21624 20490 21024 21766 20997 20837 21199 19779 20815 20970 20888 21232 20827 20671 20794 20295 19639 20834 20513 20997 20253 20378 19686 20744 20862 19892 20650 20310 21399 20385 20069 21663 20457 20263 21386 21153 20395 21654 20618 21024 21236 20493 20510 20618 21105 21115 19861 20366 21512 20505 20217 21257 20512 21669 21088 20898 21003 20934 21070 21091 21144 21167 20589 20946 20859 20469 20940 20678 20548 20386 21083 21051 20597 21301 20975 20484 20066 20890 21226 20435 20591 20874 19924 20598 21921 21056 21094 19984 20871 20873 21095 20783 20586 20297 21911 20955 21409 21420 21130 20268 21194 21125 20719 20593 20002 20515 21243 19928 20828 20844 19484 20612 19963 19963 20258 20812 21400 19900 21291 20492 20680 20229 21410 20783 20357 21172 20807 19984 21339 20835 20729 20251 20470 20044 20553 19964 21132 20665 20589 19918 21070 20159 21128 20841 20728 20539 20244 20808 20100 19300 20497 21035 20807 21209 21067 21449 20104 20784 20693 19898 20823 20869 20362 20102 20691 21343 21179 20119 20892 19731 21605 20146 20238 20927 20683 20268 20869 20449 20914 20893 20349 20374 20695 20922 20495 20790 21432 20569 21337 19650 21363 20236 20483 20972 20635 19070 19841 21179 21195 20230 22047 20336 21020 20167 20474 21599 21395 20073 20797 21022 20789 20821 21069 20962 20932 20888 20396 20560 20738 20019 20607 20648 20440 21062 20723 21759 20974 21027 20860 21041 20756 20429 21434 19796 20826 21191 20747 20379 20516 20816 20524 20548 20341 21393 20711 20837 19191 20798 20928 21138 21081 20990 21118 20950 21433 21166 20766 20813 20801 20328 21397 20171 20552 20854 20513 20458 20287 21132 20625 20826 20809 20749 20371 21308 21664 21158 20530 20191 19803 20958 20490 20666 21338 20640 20497 21353 20594 20589 20910 20763 21249 20817 20688 21106 20464 20932 20771 20465 20175 20918 20705 20511 20251 19989 20746 20652 20647 20959 20435 20457 21402 20973 21138 21051 20183 19886 20924 21119 20686 20959 20991 20912 20210 19786 20928 21260 20915 21547 21335 21100 21242 21148 21445 20259 21012 20664 20873 20876 20236 21050 19550 20345 21837 21182 21174 21065 21123 20733 21380 20416 20242 20753 20488 20484 21186 20937 20534 21161 18982 19706 20926 20593 20313 20677 20108 20516 19778 20875 20589 20005 21123 20155 19447 20597 20508 20881 20983 20584 21467 20843 21064 21211 20760 20554 20612 20172 20281 21614 20841 20960 20748 20722 21015 20738 20742 20354 21172 20811 20195 19871 20349 20757 19894 20551 20018 20900 20030 20143 21130 20697 21222 20699 20400 20748 21361 20623 19747 20554 20743 20517 20554 21376 20444 20799 20999 21414 21850 20965 19464 20343 20879 20345 20468 21108 20390 21002 20846 20159 20924 20849 20475 21246 20155 20524 20998 20548 20314 21577 20481 20238 20283 20056 20152 19831 19814 20944 20323 20978 19816 21602 20736 21094 20866 21130 19913 21593 19995 20940 21442 21201 21305 20665 20687 20794 21013 21091 19944 19924 21656 20953 20512 21475 20930 20270 20771 21379 20296 20292 20435 19992 20600 21170 20435 21241 20759 21022 20546 20430 20729 21035 20408 20888 20393 20678 21576 20177 19981 20881 20981 19975 21085 20249 20321 21302 20837 20479 20642 20533 22236 20493 20178 21540 20821 20570 20942 20325 20721 20697 20598 21069 21513 20568 20800 20182 21118 20866 21363 21062 20421 20384 20728 20261 20778 20653 19591 20876 19878 20739 20613 20057 21684 20572 21225 20681 20326 20689 20207 20757 21056 21236 20843 20914 20520 20382 20978 20413 20291 20194 20443 21056 21698 20046 20794 21151 21200 20881 21008 20336 20788 21157 20826 21245 21156 20088 19328 20069 20637 20330 19626 20578 20950 20407 21375 21087 21410 20201 20547 20892 19722 20771 20528 21033 20893 20148 20895 21068 20757 20934 20787 20942 21239 20775 20857 20794 20658 20466 21259 22002 20375 20805 21276 21501 21041 21304 21131 20950 20598 20247 21095 19843 20656 19692 20572 21134 20798 20633 20587 20799 20776 21457 21230 20858 19838 20597 20785 20670 21450 21685 21481 21394 21365 21635 21088 20252 20638 21665 20224 20893 21312 20657 20863 21148 20587 21210 20258 20484 21057 20940 20744 20248 20943 21445 20708 19894 20384 19658 20804 21339 20719 21419 21205 20570 20882 20473 20680 20430 20842 21223 20844 21141 20914 21322 20939 21024 20665 20584 20570 20156 21157 20038 19858 20482 21070 21325 20608 20254 21616 20871 20704 20637 20757 20753 21869 21274 20418 20899 21132 21488 20837 21258 20844 21379 20252 20079 21039 21363 20246 20078 20448 20911 20700 21037 20858 20512 21041 21178 20468 19681 20358 20679 20475 20462 21914 20980 20333 20692 21595 20127 20861 20849 20216 20266 21125 21360 20014 21243 20913 20929 20686 20632 21271 19830 20380 20687 20931 21402 21072 21498 20759 21451 20700 21172 20576 20818 19858 20261 20674 21128 21365 20456 20817 21615 20660 20443 20620 20299 21430 20875 20047 20397 20196 21010 20579 20943 20915 20338 19938 19631 21249 21202 20595 21368 20966 19735 20865 20754 20143 20946 19749 21090 20629 20629 20839 20700 20498 21155 20742 20920 20619 20353 20558 19836 20377 21010 20353 20982 21973 20657 19916 19911 20544 20040 20657 21505 20758 20326 20085 20195 20707 20392 19749 21117 20945 21070 21456 21188 20801 21171 20958 21139 20645 20381 20724 20649 21580 21304 20767 20609 21035 20694 20168 20743 19882 20588 21125 21564 20315 20973 20913 21229 20563 20787 20787 20825 21608 20290 20556 20661 21365 20880 20433 20184 21012 20751 20761 20690 20603 19694 20865 21017 20546 21605 20943 21390 20275 20883 20126 20405 21299 20868 20486 21362 20310 20710 20616 20826 21216 21858 20934 21435 20958 20790 20558 20505 20767 20873 20958 20078 20863 20279 21663 20981 21514 20509 20490 20340 20912 \n",
            "Matrix Size: 1000x1000 x 1000x1000 -> 1000x1000\n",
            "CPU Execution Time: 6.81958e+06 microseconds\n",
            "GPU Execution Time: 7084.62 microseconds\n",
            "Speedup (CPU time / GPU time): 962.589\n",
            "------------------------------\n"
          ]
        }
      ]
    }
  ]
}